{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68837b34-fcce-4d58-a516-f47d90c03894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK VISUALIZATION GENERATOR\n",
    "# Generates all visualizations for Model Performance page\n",
    "# ============================================================\n",
    "\n",
    "# Cell 1: Import Libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import json\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from preprocessing import create_data_generators\n",
    "from project_config import *\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cell 2: Load Model and Data\n",
    "print(\"\\nLoading model...\")\n",
    "model_path = os.path.join(MODELS_DIR, 'best_model.h5')\n",
    "model = keras.models.load_model(model_path)\n",
    "print(\"âœ“ Model loaded\")\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "_, _, test_gen = create_data_generators()\n",
    "print(f\"âœ“ Test data loaded: {test_gen.samples} images\")\n",
    "\n",
    "# Cell 3: Get Predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "test_gen.reset()\n",
    "predictions = model.predict(test_gen, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_gen.classes\n",
    "\n",
    "print(f\"âœ“ Predictions generated: {len(y_pred)} samples\")\n",
    "\n",
    "# Cell 4: Create Output Directory\n",
    "viz_dir = os.path.join(OUTPUTS_DIR, 'visualizations')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "print(f\"\\nâœ“ Output directory: {viz_dir}\")\n",
    "\n",
    "# Determine model name from metadata\n",
    "metadata_path = os.path.join(MODELS_DIR, 'metadata', 'best_model_metadata.json')\n",
    "if os.path.exists(metadata_path):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    model_name = metadata.get('best_model', 'model')\n",
    "else:\n",
    "    model_name = 'model'\n",
    "\n",
    "print(f\"âœ“ Model name: {model_name}\")\n",
    "\n",
    "# Cell 5: Generate Confusion Matrix\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1/5: CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=[c.title() for c in CLASS_NAMES],\n",
    "            yticklabels=[c.title() for c in CLASS_NAMES],\n",
    "            cbar_kws={'label': 'Proportion'})\n",
    "\n",
    "plt.title(f'Confusion Matrix - {model_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(viz_dir, f'{model_name}_confusion_matrix.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Saved: {save_path}\")\n",
    "\n",
    "# Cell 6: Generate Per-Class Metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2/5: PER-CLASS METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "report = classification_report(y_true, y_pred, \n",
    "                               target_names=[c.title() for c in CLASS_NAMES],\n",
    "                               output_dict=True)\n",
    "\n",
    "classes = [c.title() for c in CLASS_NAMES]\n",
    "metrics = {\n",
    "    'Precision': [report[c]['precision'] for c in classes],\n",
    "    'Recall': [report[c]['recall'] for c in classes],\n",
    "    'F1-Score': [report[c]['f1-score'] for c in classes]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics, index=classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "df_metrics.plot(kind='bar', ax=ax, width=0.8, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "ax.set_title(f'Per-Class Metrics - {model_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(viz_dir, f'{model_name}_per_class_metrics.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Saved: {save_path}\")\n",
    "\n",
    "# Cell 7: Generate Confidence Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3/5: CONFIDENCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "confidence_scores = np.max(predictions, axis=1)\n",
    "correct_mask = (y_true == y_pred)\n",
    "correct_confidence = confidence_scores[correct_mask]\n",
    "incorrect_confidence = confidence_scores[~correct_mask]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(correct_confidence, bins=30, alpha=0.7, label='Correct', \n",
    "             color='green', edgecolor='black')\n",
    "axes[0].hist(incorrect_confidence, bins=30, alpha=0.7, label='Incorrect', \n",
    "             color='red', edgecolor='black')\n",
    "axes[0].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Confidence Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "data_to_plot = [correct_confidence, incorrect_confidence]\n",
    "axes[1].boxplot(data_to_plot, labels=['Correct', 'Incorrect'],\n",
    "                patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', edgecolor='black'),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[1].set_title('Confidence Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Confidence Score', fontsize=12)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(viz_dir, f'{model_name}_confidence_analysis.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Saved: {save_path}\")\n",
    "\n",
    "# Cell 8: Generate Misclassifications\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4/5: MISCLASSIFICATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "misclassified_indices = np.where(y_true != y_pred)[0]\n",
    "\n",
    "if len(misclassified_indices) > 0:\n",
    "    num_samples = min(12, len(misclassified_indices))\n",
    "    sample_indices = np.random.choice(misclassified_indices, num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    fig.suptitle(f'Misclassified Examples - {model_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    file_paths = test_gen.filepaths\n",
    "    \n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < len(sample_indices):\n",
    "            sample_idx = sample_indices[idx]\n",
    "            \n",
    "            # Load image\n",
    "            img_path = file_paths[sample_idx]\n",
    "            img = keras.utils.load_img(img_path, target_size=IMG_SIZE)\n",
    "            img_array = keras.utils.img_to_array(img) / 255.0\n",
    "            \n",
    "            # Get predictions\n",
    "            true_label = CLASS_NAMES[y_true[sample_idx]].title()\n",
    "            pred_label = CLASS_NAMES[y_pred[sample_idx]].title()\n",
    "            confidence = predictions[sample_idx][y_pred[sample_idx]]\n",
    "            \n",
    "            # Display\n",
    "            ax.imshow(img_array)\n",
    "            ax.set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.2f})',\n",
    "                        fontsize=10, color='red')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(viz_dir, f'{model_name}_misclassifications.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"âœ“ Saved: {save_path}\")\n",
    "else:\n",
    "    print(\"âœ“ No misclassifications (perfect model!)\")\n",
    "\n",
    "# Cell 9: Generate Training History (Placeholder)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5/5: TRAINING HISTORY (PLACEHOLDER)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create placeholder since we don't have actual training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(f'{model_name} - Training History (Simulated)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Simulate training curves\n",
    "epochs = np.arange(1, 21)\n",
    "train_acc = 0.4 + (0.45 * (1 - np.exp(-epochs/5)))\n",
    "val_acc = 0.4 + (0.4 * (1 - np.exp(-epochs/5)))\n",
    "\n",
    "train_loss = 1.6 * np.exp(-epochs/8) + 0.3\n",
    "val_loss = 1.6 * np.exp(-epochs/7) + 0.4\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(epochs, train_acc, label='Train', linewidth=2)\n",
    "axes[0, 0].plot(epochs, val_acc, label='Validation', linewidth=2)\n",
    "axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(epochs, train_loss, label='Train', linewidth=2)\n",
    "axes[0, 1].plot(epochs, val_loss, label='Validation', linewidth=2)\n",
    "axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision/Recall (simulated)\n",
    "axes[1, 0].plot(epochs, train_acc * 0.95, label='Train', linewidth=2)\n",
    "axes[1, 0].plot(epochs, val_acc * 0.95, label='Validation', linewidth=2)\n",
    "axes[1, 0].set_title('Model Precision', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(epochs, train_acc * 0.93, label='Train', linewidth=2)\n",
    "axes[1, 1].plot(epochs, val_acc * 0.93, label='Validation', linewidth=2)\n",
    "axes[1, 1].set_title('Model Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(viz_dir, f'{model_name}_training_history.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Saved: {save_path}\")\n",
    "\n",
    "# Cell 10: Save Classification Report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "reports_dir = os.path.join(OUTPUTS_DIR, 'reports')\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "report_path = os.path.join(reports_dir, f'{model_name}_classification_report.json')\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=4)\n",
    "\n",
    "print(f\"âœ“ Saved: {report_path}\")\n",
    "\n",
    "# Cell 11: Final Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ALL VISUALIZATIONS GENERATED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "generated_files = [\n",
    "    f'{model_name}_confusion_matrix.png',\n",
    "    f'{model_name}_per_class_metrics.png',\n",
    "    f'{model_name}_confidence_analysis.png',\n",
    "    f'{model_name}_misclassifications.png',\n",
    "    f'{model_name}_training_history.png'\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸ“ Generated files in: {viz_dir}\\n\")\n",
    "\n",
    "for file in generated_files:\n",
    "    file_path = os.path.join(viz_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        size_kb = os.path.getsize(file_path) / 1024\n",
    "        print(f\"  âœ… {file} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file} - MISSING\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ REFRESH STREAMLIT NOW!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThe Model Performance page should now show all visualizations!\")\n",
    "print(\"\\nIf Streamlit is running:\")\n",
    "print(\"  1. Just refresh the browser (F5)\")\n",
    "print(\"  2. Navigate to Model Performance page\")\n",
    "print(\"  3. All tabs should show visualizations now!\")\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Garbage Classification)",
   "language": "python",
   "name": "garbage-classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
