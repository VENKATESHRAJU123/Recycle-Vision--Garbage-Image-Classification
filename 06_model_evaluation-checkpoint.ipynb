{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50431b7f-9f56-4939-b40c-b61d37b9803b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GARBAGE CLASSIFICATION - MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "âœ“ All libraries imported successfully!\n",
      "  NumPy version: 2.2.6\n",
      "  Pandas version: 2.3.3\n",
      "  TensorFlow version: 3.13.0\n",
      "\n",
      "============================================================\n",
      "LOADING TEST DATA\n",
      "============================================================\n",
      "Found 1819 images belonging to 6 classes.\n",
      "Found 325 images belonging to 6 classes.\n",
      "Found 383 images belonging to 6 classes.\n",
      "âœ“ Data generators created!\n",
      "  Training samples: 1819\n",
      "  Validation samples: 325\n",
      "  Test samples: 383\n",
      "\n",
      "âœ“ Test data loaded successfully!\n",
      "\n",
      "ðŸ“Š Test Set Information:\n",
      "  Total samples: 383\n",
      "  Number of classes: 6\n",
      "  Batch size: 32\n",
      "  Image size: (224, 224)\n",
      "\n",
      "  Classes: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "\n",
      "  Class distribution in test set:\n",
      "    cardboard    â†’   61 images (15.9%)\n",
      "    glass        â†’   76 images (19.8%)\n",
      "    metal        â†’   62 images (16.2%)\n",
      "    paper        â†’   90 images (23.5%)\n",
      "    plastic      â†’   73 images (19.1%)\n",
      "    trash        â†’   21 images (5.5%)\n",
      "\n",
      "============================================================\n",
      "CHECKING FOR TRAINED MODELS\n",
      "============================================================\n",
      "\n",
      "Scanning for model files...\n",
      "  âŒ Missing: Baseline CNN\n",
      "  âŒ Missing: MobileNetV2\n",
      "  âŒ Missing: EfficientNetB0\n",
      "  âŒ Missing: ResNet50\n",
      "  âŒ Missing: Best Model\n",
      "\n",
      "============================================================\n",
      "âš ï¸ NO TRAINED MODELS FOUND!\n",
      "============================================================\n",
      "\n",
      "ðŸ“ To train models, run one of these notebooks:\n",
      "  1. notebooks/04_model_training_baseline.ipynb\n",
      "     (Trains baseline CNN model - ~30 minutes)\n",
      "\n",
      "  2. notebooks/04_quick_training.ipynb\n",
      "     (Quick training with MobileNetV2 - ~15 minutes)\n",
      "\n",
      "  3. notebooks/05_transfer_learning_models.ipynb\n",
      "     (Trains all models - ~2-4 hours)\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No trained models available for evaluation. Please train models first using one of the training notebooks.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m     (Trains all models - ~2-4 hours)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    113\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo trained models available for evaluation. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    114\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease train models first using one of the training notebooks.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    115\u001b[39m     )\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(available_models)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m trained model(s)!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_models:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No trained models available for evaluation. Please train models first using one of the training notebooks."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model evaluation and performance analysis\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from tensorflow import keras\n",
    "import json\n",
    "from config import *\n",
    "\n",
    "def evaluate_model(model, test_generator, model_name='model'):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        test_generator: Test data generator\n",
    "        model_name: Name for saving results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Get predictions\n",
    "    test_generator.reset()\n",
    "    predictions = model.predict(test_generator, verbose=1)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Get true labels\n",
    "    y_true = test_generator.classes\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Overall Metrics:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nðŸ“‹ Classification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(report)\n",
    "    \n",
    "    # Save classification report\n",
    "    report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    report_path = os.path.join(OUTPUTS_DIR, 'reports', f'{model_name}_classification_report.json')\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report_dict, f, indent=4)\n",
    "    print(f\"\\nâœ“ Report saved to: {report_path}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plot_confusion_matrix(cm, class_names, model_name)\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    plot_per_class_metrics(report_dict, model_name)\n",
    "    \n",
    "    # Prediction confidence analysis\n",
    "    analyze_prediction_confidence(predictions, y_true, y_pred, class_names, model_name)\n",
    "    \n",
    "    # Misclassification analysis\n",
    "    analyze_misclassifications(test_generator, y_true, y_pred, predictions, class_names, model_name)\n",
    "    \n",
    "    # Return metrics\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': report_dict\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, model_name):\n",
    "    \"\"\"Plot confusion matrix heatmap\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Normalize\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Proportion'})\n",
    "    \n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(OUTPUTS_DIR, 'visualizations', f'{model_name}_confusion_matrix.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Confusion matrix saved to: {save_path}\")\n",
    "\n",
    "def plot_per_class_metrics(report_dict, model_name):\n",
    "    \"\"\"Plot per-class precision, recall, and F1-score\"\"\"\n",
    "    # Extract metrics\n",
    "    classes = [k for k in report_dict.keys() if k not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "    \n",
    "    metrics = {\n",
    "        'Precision': [report_dict[c]['precision'] for c in classes],\n",
    "        'Recall': [report_dict[c]['recall'] for c in classes],\n",
    "        'F1-Score': [report_dict[c]['f1-score'] for c in classes]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_metrics = pd.DataFrame(metrics, index=classes)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    df_metrics.plot(kind='bar', ax=ax, width=0.8, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    ax.set_title(f'Per-Class Metrics - {model_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.2f', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(OUTPUTS_DIR, 'visualizations', f'{model_name}_per_class_metrics.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Per-class metrics saved to: {save_path}\")\n",
    "\n",
    "def analyze_prediction_confidence(predictions, y_true, y_pred, class_names, model_name):\n",
    "    \"\"\"Analyze prediction confidence distribution\"\"\"\n",
    "    # Get confidence scores\n",
    "    confidence_scores = np.max(predictions, axis=1)\n",
    "    \n",
    "    # Separate correct and incorrect predictions\n",
    "    correct_mask = (y_true == y_pred)\n",
    "    correct_confidence = confidence_scores[correct_mask]\n",
    "    incorrect_confidence = confidence_scores[~correct_mask]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(correct_confidence, bins=30, alpha=0.7, label='Correct', \n",
    "                 color='green', edgecolor='black')\n",
    "    axes[0].hist(incorrect_confidence, bins=30, alpha=0.7, label='Incorrect', \n",
    "                 color='red', edgecolor='black')\n",
    "    axes[0].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Confidence Score', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    data_to_plot = [correct_confidence, incorrect_confidence]\n",
    "    axes[1].boxplot(data_to_plot, labels=['Correct', 'Incorrect'],\n",
    "                    patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightblue', edgecolor='black'),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "    axes[1].set_title('Confidence Score Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Confidence Score', fontsize=12)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(OUTPUTS_DIR, 'visualizations', f'{model_name}_confidence_analysis.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Confidence analysis saved to: {save_path}\")\n",
    "    print(f\"\\n  Average confidence (correct):   {np.mean(correct_confidence):.4f}\")\n",
    "    print(f\"  Average confidence (incorrect): {np.mean(incorrect_confidence):.4f}\")\n",
    "\n",
    "def analyze_misclassifications(test_generator, y_true, y_pred, predictions, class_names, model_name):\n",
    "    \"\"\"Visualize most common misclassifications\"\"\"\n",
    "    # Find misclassified samples\n",
    "    misclassified_indices = np.where(y_true != y_pred)[0]\n",
    "    \n",
    "    if len(misclassified_indices) == 0:\n",
    "        print(\"\\nâœ“ No misclassifications! Perfect model!\")\n",
    "        return\n",
    "    \n",
    "    # Get file paths\n",
    "    file_paths = test_generator.filepaths\n",
    "    \n",
    "    # Select random misclassified samples (max 12)\n",
    "    num_samples = min(12, len(misclassified_indices))\n",
    "    sample_indices = np.random.choice(misclassified_indices, num_samples, replace=False)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    fig.suptitle(f'Misclassified Examples - {model_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < len(sample_indices):\n",
    "            sample_idx = sample_indices[idx]\n",
    "            \n",
    "            # Load image\n",
    "            img_path = file_paths[sample_idx]\n",
    "            img = keras.utils.load_img(img_path, target_size=IMG_SIZE)\n",
    "            img_array = keras.utils.img_to_array(img) / 255.0\n",
    "            \n",
    "            # Get predictions\n",
    "            true_label = class_names[y_true[sample_idx]]\n",
    "            pred_label = class_names[y_pred[sample_idx]]\n",
    "            confidence = predictions[sample_idx][y_pred[sample_idx]]\n",
    "            \n",
    "            # Display\n",
    "            ax.imshow(img_array)\n",
    "            ax.set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.2f})',\n",
    "                        fontsize=10, color='red')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(OUTPUTS_DIR, 'visualizations', f'{model_name}_misclassifications.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Misclassifications saved to: {save_path}\")\n",
    "    print(f\"  Total misclassified: {len(misclassified_indices)} / {len(y_true)}\")\n",
    "\n",
    "def compare_models(results_dict):\n",
    "    \"\"\"\n",
    "    Compare multiple models\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary with model names as keys and results as values\n",
    "    \"\"\"\n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for model_name, results in results_dict.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1-Score': results['f1_score']\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    df_comparison = df_comparison.sort_values('F1-Score', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON - TEST SET RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(df_comparison))\n",
    "    width = 0.2\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        offset = width * (i - 1.5)\n",
    "        bars = ax.bar(x + offset, df_comparison[metric], width, \n",
    "                     label=metric, color=colors[i], edgecolor='black', linewidth=1.2)\n",
    "        ax.bar_label(bars, fmt='%.3f', fontsize=8)\n",
    "    \n",
    "    ax.set_title('Model Performance Comparison (Test Set)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_comparison['Model'], rotation=45, ha='right')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(OUTPUTS_DIR, 'visualizations', 'model_comparison_test.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save comparison\n",
    "    csv_path = os.path.join(OUTPUTS_DIR, 'reports', 'model_comparison_test.csv')\n",
    "    df_comparison.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nâœ“ Comparison saved to: {csv_path}\")\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = df_comparison.iloc[0]\n",
    "    print(f\"\\nðŸ† BEST MODEL: {best_model['Model']}\")\n",
    "    print(f\"   F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "    print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"âœ“ Evaluation module loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f1891-8b56-4532-b9a4-29e71a40e838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41270fa1-d4e2-4edf-88f2-62bdc255dc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Garbage Classification)",
   "language": "python",
   "name": "garbage-classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
