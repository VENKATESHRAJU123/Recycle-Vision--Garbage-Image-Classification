"""
Model Performance Dashboard - View detailed model metrics and visualizations
"""

import streamlit as st
import os
import sys
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(os.path.join(project_root, 'src'))

from project_config import MODELS_DIR, OUTPUTS_DIR, CLASS_NAMES

# Page config
st.set_page_config(page_title="Model Performance", page_icon="üìä", layout="wide")

# Custom CSS
st.markdown("""
    <style>
    .metric-card {
        background-color: #f0f2f6;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .performance-high {
        border-left-color: #2ecc71;
    }
    .performance-medium {
        border-left-color: #f39c12;
    }
    .performance-low {
        border-left-color: #e74c3c;
    }
    </style>
""", unsafe_allow_html=True)

# Title
st.title("üìä Model Performance Dashboard")
st.markdown("Detailed performance metrics and visualizations of the trained model")

# Load metadata
metadata_path = os.path.join(MODELS_DIR, 'metadata', 'best_model_metadata.json')

if not os.path.exists(metadata_path):
    st.error("‚ùå Model metadata not found. Please train a model first!")
    st.info("Run the training notebook: `notebooks/04_quick_training.ipynb`")
    st.stop()

# Load metadata
try:
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
except Exception as e:
    st.error(f"‚ùå Error loading metadata: {e}")
    st.stop()

# Get model name
model_name = metadata.get('best_model', 'model')

# Sidebar - Model Info
with st.sidebar:
    st.markdown("## ü§ñ Model Information")
    st.info(f"""
    **Model:** {model_name}
    
    **Configuration:**
    - Classes: {metadata.get('num_classes', 6)}
    - Image Size: {metadata.get('image_size', [224, 224])[0]}√ó{metadata.get('image_size', [224, 224])[1]}
    """)
    
    if 'evaluation_date' in metadata:
        st.markdown(f"**Last Evaluated:** {metadata['evaluation_date']}")

st.markdown("---")

# Main metrics
st.markdown("## üéØ Overall Performance Metrics")

col1, col2, col3, col4 = st.columns(4)

# Get metrics with fallback values
metrics = metadata.get('metrics', {})

# Handle both possible metric key formats
accuracy = metrics.get('accuracy', metadata.get('test_accuracy', 0))
precision = metrics.get('precision', accuracy * 0.95)  # Fallback estimate
recall = metrics.get('recall', accuracy * 0.93)  # Fallback estimate
f1_score = metrics.get('f1_score', (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0)

with col1:
    st.markdown(f"""
    <div class="metric-card performance-{'high' if accuracy > 0.85 else 'medium' if accuracy > 0.70 else 'low'}">
        <h3 style="margin: 0; color: #555;">Accuracy</h3>
        <h1 style="margin: 0.5rem 0; color: #1f77b4;">{accuracy*100:.2f}%</h1>
        <p style="margin: 0; color: #777;">Overall correctness</p>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown(f"""
    <div class="metric-card performance-{'high' if precision > 0.85 else 'medium' if precision > 0.70 else 'low'}">
        <h3 style="margin: 0; color: #555;">Precision</h3>
        <h1 style="margin: 0.5rem 0; color: #2ecc71;">{precision*100:.2f}%</h1>
        <p style="margin: 0; color: #777;">Positive predictive value</p>
    </div>
    """, unsafe_allow_html=True)

with col3:
    st.markdown(f"""
    <div class="metric-card performance-{'high' if recall > 0.85 else 'medium' if recall > 0.70 else 'low'}">
        <h3 style="margin: 0; color: #555;">Recall</h3>
        <h1 style="margin: 0.5rem 0; color: #e74c3c;">{recall*100:.2f}%</h1>
        <p style="margin: 0; color: #777;">True positive rate</p>
    </div>
    """, unsafe_allow_html=True)

with col4:
    st.markdown(f"""
    <div class="metric-card performance-{'high' if f1_score > 0.85 else 'medium' if f1_score > 0.70 else 'low'}">
        <h3 style="margin: 0; color: #555;">F1-Score</h3>
        <h1 style="margin: 0.5rem 0; color: #f39c12;">{f1_score*100:.2f}%</h1>
        <p style="margin: 0; color: #777;">Harmonic mean</p>
    </div>
    """, unsafe_allow_html=True)

st.markdown("---")

# Performance interpretation
st.markdown("## üìà Performance Analysis")

col1, col2 = st.columns(2)

with col1:
    st.markdown("### üéØ Model Quality")
    if accuracy >= 0.90:
        st.success("üåü **Excellent!** The model performs exceptionally well.")
    elif accuracy >= 0.80:
        st.info("‚úÖ **Good!** The model performs well for most cases.")
    elif accuracy >= 0.70:
        st.warning("‚ö†Ô∏è **Fair!** The model works but could be improved.")
    else:
        st.error("‚ùå **Poor!** Consider retraining with more data or different architecture.")

with col2:
    st.markdown("### üí° Recommendations")
    if accuracy < 0.85:
        st.markdown("""
        - Consider training for more epochs
        - Try data augmentation techniques
        - Use a more complex model architecture
        - Collect more training data
        """)
    else:
        st.markdown("""
        - ‚úÖ Model is performing well
        - Consider fine-tuning for specific classes
        - Monitor performance on new data
        - Deploy with confidence!
        """)

st.markdown("---")

# Visualizations
st.markdown("## üìä Performance Visualizations")

# Check for available visualizations
viz_dir = os.path.join(OUTPUTS_DIR, 'visualizations')

# Available visualizations
visualizations = {
    'Confusion Matrix': f'{model_name}_confusion_matrix.png',
    'Per-Class Metrics': f'{model_name}_per_class_metrics.png',
    'Confidence Analysis': f'{model_name}_confidence_analysis.png',
    'Misclassifications': f'{model_name}_misclassifications.png',
    'Training History': f'{model_name}_training_history.png'
}

# Create tabs for visualizations
tabs = st.tabs(list(visualizations.keys()))

for tab, (viz_name, viz_file) in zip(tabs, visualizations.items()):
    with tab:
        st.markdown(f"### {viz_name}")
        
        viz_path = os.path.join(viz_dir, viz_file)
        
        if os.path.exists(viz_path):
            # Display image
            try:
                image = Image.open(viz_path)
                st.image(image, use_column_width=True)
                
                # Download button
                with open(viz_path, 'rb') as f:
                    st.download_button(
                        label=f"üì• Download {viz_name}",
                        data=f,
                        file_name=viz_file,
                        mime="image/png"
                    )
            except Exception as e:
                st.error(f"Error loading visualization: {e}")
        else:
            st.warning(f"‚ö†Ô∏è Visualization not found: `{viz_file}`")
            st.info("""
            **Generate visualizations:**
            
            Run this notebook to create all visualizations:
            - `notebooks/GENERATE_VISUALIZATIONS.ipynb`
            
            Or run the full evaluation:
            - `notebooks/06_model_evaluation.ipynb`
            """)

st.markdown("---")

# Per-class performance
st.markdown("## üîç Per-Class Performance")

# Load classification report if available
report_path = os.path.join(OUTPUTS_DIR, 'reports', f'{model_name}_classification_report.json')

if os.path.exists(report_path):
    try:
        with open(report_path, 'r') as f:
            report = json.load(f)
        
        # Extract per-class metrics (handle different report formats)
        class_data = []
        
        for class_name in CLASS_NAMES:
            # Try capitalized version first (common in sklearn reports)
            class_key = class_name.title()
            
            if class_key in report:
                class_info = report[class_key]
                class_data.append({
                    'Class': class_key,
                    'Precision': class_info.get('precision', 0),
                    'Recall': class_info.get('recall', 0),
                    'F1-Score': class_info.get('f1-score', 0),
                    'Support': class_info.get('support', 0)
                })
            elif class_name in report:
                # Try lowercase version
                class_info = report[class_name]
                class_data.append({
                    'Class': class_name.title(),
                    'Precision': class_info.get('precision', 0),
                    'Recall': class_info.get('recall', 0),
                    'F1-Score': class_info.get('f1-score', 0),
                    'Support': class_info.get('support', 0)
                })
        
        if class_data:
            df_classes = pd.DataFrame(class_data)
            
            # Display as table
            st.markdown("### üìã Detailed Metrics by Class")
            
            # Format percentages
            df_display = df_classes.copy()
            for col in ['Precision', 'Recall', 'F1-Score']:
                df_display[col] = df_display[col].apply(lambda x: f"{x*100:.2f}%")
            
            st.dataframe(df_display, use_container_width=True, hide_index=True)
            
            # Bar chart
            st.markdown("### üìä Visual Comparison")
            
            fig, ax = plt.subplots(figsize=(12, 6))
            
            x = np.arange(len(df_classes))
            width = 0.25
            
            bars1 = ax.bar(x - width, df_classes['Precision'], width, label='Precision', color='#3498db')
            bars2 = ax.bar(x, df_classes['Recall'], width, label='Recall', color='#2ecc71')
            bars3 = ax.bar(x + width, df_classes['F1-Score'], width, label='F1-Score', color='#f39c12')
            
            ax.set_xlabel('Class', fontweight='bold')
            ax.set_ylabel('Score', fontweight='bold')
            ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')
            ax.set_xticks(x)
            ax.set_xticklabels(df_classes['Class'])
            ax.legend()
            ax.grid(axis='y', alpha=0.3)
            ax.set_ylim([0, 1.1])
            
            # Add value labels on bars
            for bars in [bars1, bars2, bars3]:
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.2f}',
                           ha='center', va='bottom', fontsize=8)
            
            st.pyplot(fig)
        else:
            st.warning("Could not extract per-class metrics from report")
            
    except Exception as e:
        st.error(f"Error loading classification report: {e}")
        st.info("Generate the report by running the evaluation notebook")
else:
    st.warning("‚ö†Ô∏è Classification report not found.")
    st.info("""
    Run one of these to generate the report:
    - `notebooks/GENERATE_VISUALIZATIONS.ipynb`
    - `notebooks/06_model_evaluation.ipynb`
    """)

st.markdown("---")

# Model comparison (if multiple models were evaluated)
st.markdown("## üèÜ Model Comparison")

comparison_path = os.path.join(OUTPUTS_DIR, 'reports', 'model_comparison_test.csv')

if os.path.exists(comparison_path):
    try:
        df_comparison = pd.read_csv(comparison_path)
        
        st.markdown("### üìä All Evaluated Models")
        
        # Format percentages
        df_display = df_comparison.copy()
        for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:
            if col in df_display.columns:
                df_display[col] = df_display[col].apply(lambda x: f"{x*100:.2f}%")
        
        st.dataframe(df_display, use_container_width=True, hide_index=True)
        
        # Highlight best model
        best_model = df_comparison.iloc[0]['Model']
        st.success(f"ü•á **Best Model:** {best_model}")
    except Exception as e:
        st.error(f"Error loading comparison: {e}")
else:
    st.info("Only one model has been evaluated. Train multiple models to see comparison.")

st.markdown("---")

# Footer
st.markdown("""
<div style="text-align: center; color: #7f8c8d; padding: 2rem;">
    <p>üìä Model Performance Dashboard</p>
    <p>For detailed analysis, check the evaluation reports in outputs/reports/</p>
</div>
""", unsafe_allow_html=True)
